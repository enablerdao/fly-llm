# LiteLLM Proxy Environment Variables
# Copy this file to .env and fill in your values

# ======== API Keys for LLM Providers ========
# OpenAI API Key
OPENAI_API_KEY=sk-your-openai-api-key

# Anthropic API Key (for Claude models)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key

# Mistral AI API Key
MISTRAL_API_KEY=your-mistral-api-key

# DeepSeek API Key
DEEPSEEK_API_KEY=your-deepseek-api-key

# Google API Key (for Gemini models)
GOOGLE_API_KEY=your-google-api-key

# Azure OpenAI Settings
AZURE_API_KEY=your-azure-api-key
AZURE_API_BASE=https://your-resource-name.openai.azure.com
AZURE_API_VERSION=2023-05-15

# ======== Self-hosted LLM Settings ========
# Ollama API Base URL (for local LLMs)
OLLAMA_API_BASE=http://your-ollama-server:11434

# vLLM API Base URL (for high-performance inference)
VLLM_API_BASE=http://your-vllm-server:8000

# Local LLM Server with OpenAI-compatible API
LOCAL_LLM_API_BASE=http://your-local-server:8080

# Rakuten LLM Server
RAKUTEN_LLM_API_BASE=http://rakuten-llm:8000

# ======== Auto Model Selection Settings ========
# These settings control the behavior of the "auto" model selection feature

# Default model to use when auto-selection can't determine the best model
AUTO_DEFAULT_MODEL=claude-3-haiku

# Cost threshold for using more expensive models (in USD per 1K tokens)
AUTO_COST_THRESHOLD=0.01

# Maximum tokens to generate for auto-selected models
AUTO_MAX_TOKENS=2000

# Language-specific model preferences
# Format: language:preferred_model
AUTO_LANGUAGE_MODELS=japanese:claude-3-haiku,chinese:claude-3-haiku,korean:claude-3-haiku

# Task-specific model preferences
# Format: task:preferred_model
AUTO_TASK_MODELS=coding:gpt-3.5-turbo,creative:claude-3-sonnet,reasoning:gpt-4,ecommerce:rakuten-llm

# ======== Proxy Server Settings ========
# Port for the server to listen on
PORT=8080

# Redis cache settings
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=

# Enable response caching
ENABLE_CACHING=true
CACHE_TTL=3600

# Budget management
DEFAULT_BUDGET_LIMIT=10.0

# ======== Security Settings ========
# Admin API key (generate a secure random key)
ADMIN_API_KEY=sk-your-admin-api-key

# Enable rate limiting
ENABLE_RATE_LIMIT=true
RATE_LIMIT=100
RATE_LIMIT_TIMEFRAME=60

# ======== Logging Settings ========
# Log level (debug, info, warning, error)
LOG_LEVEL=info

# Enable detailed request/response logging
DETAILED_LOGGING=false

# ======== Auto Model Selection Improvement ========
# These settings are used to improve the auto model selection over time

# Enable learning from user feedback
AUTO_LEARN_FROM_FEEDBACK=true

# Enable collecting usage statistics for model performance
AUTO_COLLECT_STATS=true

# Model for analyzing and improving auto-selection
AUTO_IMPROVEMENT_MODEL=gpt-4

# System prompt for the auto-improvement model
AUTO_IMPROVEMENT_PROMPT="You are an AI assistant that helps improve model selection logic. Analyze the following request and response data to determine if the selected model was appropriate. Consider factors like language, complexity, code content, and response quality. Suggest improvements to the selection algorithm."